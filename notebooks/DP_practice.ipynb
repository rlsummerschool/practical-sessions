{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hr2tPjlrf-rP"
   },
   "source": [
    "# Practice Session 1: Policy and Value Iteration\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rlsummerschool/practical-sessions/blob/master/notebooks/DP_practice.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYZ_hFDkXuY0"
   },
   "outputs": [],
   "source": [
    "# Set up the environment\n",
    "!pip install \"git+https://github.com/rlsummerschool/practical-sessions.git\" --quiet\n",
    "\n",
    "\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFtCdzEI9gHZ"
   },
   "source": [
    "## RL development in Python\n",
    "\n",
    "Large RL projects are usually developed locally, as any other Python package, in OOP, version controlled by Git (see [example](https://github.com/rlsummerschool/practical-sessions/blob/master/rlss_practice/environments.py)).\n",
    "\n",
    "[Jupyter](https://jupyter.org/) notebooks, like this one, are collections of text cells and code cells, that can be executed with an interactive interpreter.\n",
    "Colab notebooks are Jupyter notebooks whose interpreter runs in a runtime that is hosted by Google."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqPq-DrFG3se"
   },
   "source": [
    "### Environments and tasks\n",
    "\n",
    "An MDP is a model of Reinforcement Learning tasks. It comprises of:\n",
    "* a **state space** $\\mathcal{S}$\n",
    "* a **action space** $\\mathcal{A}$\n",
    "* a **starting-state distribution** $\\nu_0(s)$\n",
    "* a **reward function** $r: \\mathcal{S}\\times\\mathcal{A}\\rightarrow [0,1]$ (or vector $r\\in[0,1]^{|\\mathcal{S}|\\cdot|\\mathcal{A}|}$)\n",
    "* a **transition function** $p: \\mathcal{S}\\times\\mathcal{A}\\rightarrow \\Delta_{\\mathcal{S}}$ (or matrix $p \\in\\mathbb{R}^{|\\mathcal{S}|\\cdot|\\mathcal{A}|\\times|\\mathcal{S}|}$)\n",
    "\n",
    "where $\\Delta_{\\mathcal{S}} = \\{q\\in\\mathbb{R}^{|\\mathcal{S}|} \\,\\,|\\,\\, \\sum_{s\\in\\mathcal{S}}q(s) = 1,\\, q(s)\\geq 0 \\text{ for } s\\in\\mathcal{S}\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsQticuSJmdO"
   },
   "source": [
    "Interaction between an agent (or decision maker) and some environment (or task) in some round $t$:\n",
    "\n",
    "<div>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=11KyhHxuaileBEJ9fFZmIS5-GzncQgyqt\" alt=\"agent-environment interaction\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "The agent is equipped with a policy $\\pi$ mapping states to actions (or a distribution over actions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ob8WriuZLh-g"
   },
   "source": [
    "### The Gym(nasium) interface\n",
    "\n",
    "[Gymnasium](https://gymnasium.farama.org/) is a standard API for Decision Processes, based on OpenAI's Gym library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JgaqoJNTOi6o"
   },
   "outputs": [],
   "source": [
    "# Standard import\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDwwk5BL1eRQ"
   },
   "source": [
    "The library also defines a number of classic benchmarks [Atari](https://gymnasium.farama.org/environments/atari/) games, [MuJoCo](https://gymnasium.farama.org/environments/mujoco/) simulations, and [ToyText](https://gymnasium.farama.org/environments/toy_text/), minimal environments for debugging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DlzPIsH23hpE"
   },
   "outputs": [],
   "source": [
    "# A registered environment (for example Cliff Walking) can be instantiated with\n",
    "env = gym.make(\"CliffWalking-v0\")\n",
    "\n",
    "# Important properties:\n",
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXhGGkAI7XEh"
   },
   "source": [
    "We only consider discrete state and action spaces. See the possible alternatives: `dir(gym.spaces)`\n",
    "\n",
    "Environments can be fully observable, partially observable or non-stationary. All these variants fit the same environment interface.\n",
    "We should know which class our environment belongs to.\n",
    "Today, we only consider stationary and fully observable MDPs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNcySyOGACgZ"
   },
   "source": [
    "The environment interface allows  to sample initial states and transition with `env.reset()` and `env.step(action)`. Every method is well documented (see `help(gym.Env.step)`).\n",
    "\n",
    "Try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wnymj388egTo"
   },
   "outputs": [],
   "source": [
    "observation, info = env.reset()\n",
    "print(f\"Initial observation: {observation}\")\n",
    "\n",
    "action = 0  # any action\n",
    "\n",
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "print(f\"Observation: {observation}, reward {reward}\")\n",
    "\n",
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "print(f\"Observation: {observation}, reward {reward}\")\n",
    "\n",
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "print(f\"Observation: {observation}, reward {reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xffKa7UfiOC"
   },
   "source": [
    "Custom environments can be created by subclassing the `Env` class.\n",
    "\n",
    "Environments can be also modified with wrappers. See the predefined Gymnasium [Wrappers](https://gymnasium.farama.org/api/wrappers/#gymnasium-wrappers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHgub_3DH-9V"
   },
   "source": [
    "## Setting up our RL task\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QotsgLalKv4"
   },
   "source": [
    "We experiment with a simple grid-world environment, based on the implementation in [minigrid](https://minigrid.farama.org/environments/minigrid/). The environment is configured and observations are transformed appropriately (you already know how)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dzPxVxymjlZr"
   },
   "outputs": [],
   "source": [
    "from rlss_practice.environments import Room, Rooms, MinigridBase\n",
    "\n",
    "\n",
    "# Initializing the environment\n",
    "env = Room(\n",
    "    failure=0.0,\n",
    "    agent_start_pos=(1, 1),\n",
    "    agent_start_dir=0,\n",
    "    size=5,\n",
    ")\n",
    "print(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AA74pMHsl_be"
   },
   "source": [
    "In the ASCII representation, > is the agent, facing right, G is the goal, and W are walls.\n",
    "\n",
    "Understand you own environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pY4EV-n0mH-d"
   },
   "outputs": [],
   "source": [
    "print(\"Action space:\", env.action_space)\n",
    "print(\"Observation space:\", env.observation_space, end=\"\\n\\n\")\n",
    "#print(help(MinigridBase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tk3VBz-ZmsvM"
   },
   "source": [
    "When you checkout `help(env)`:\n",
    "\n",
    "    class MinigridBase\n",
    "       MinigridBase(minigrid: minigrid.minigrid_env.MiniGridEnv, seed: int, failure=0.0)\n",
    "\n",
    "       Base class for minigrid environments with explicit transition and reward functions.\n",
    "\n",
    "       The agent is rewarded upon reaching the goal location.\n",
    "\n",
    "       Action space:\n",
    "\n",
    "       | Num | Name         | Action       |\n",
    "       |-----|--------------|--------------|\n",
    "       | 0   | left         | Turn left    |\n",
    "       | 1   | right        | Turn right   |\n",
    "       | 2   | forward      | Move forward |\n",
    "\n",
    "       Observation space:\n",
    "\n",
    "       | Name | Description             |\n",
    "       |------|-------------------------|\n",
    "       | x    | x coordinate            |\n",
    "       | y    | y coordinate (downward) |\n",
    "       | dir  | cardinal direction      |\n",
    "\n",
    "       The transition function is stored in `T`,\n",
    "       where `T[state][action][next_state]` is the transition probability.\n",
    "       The reward function is `R`. `R[state][action]` contains a reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOD43HaO2M6X"
   },
   "source": [
    "### Demo\n",
    "\n",
    "Here's a rollout loop, for a single trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ugRanvqJo4RW"
   },
   "outputs": [],
   "source": [
    "# Test it\n",
    "done = False\n",
    "observation, info = env.reset()\n",
    "print(\"Initial observation:\", observation)\n",
    "print(env)\n",
    "\n",
    "# Steps\n",
    "while not done:\n",
    "\n",
    "    try:\n",
    "      # Action selection\n",
    "\n",
    "      action = int(input(\"Action: \"))\n",
    "\n",
    "      # Transition\n",
    "      observation, reward, terminated, truncated, info = env.step(action)\n",
    "      print(terminated)\n",
    "      print(truncated)\n",
    "      done = terminated or truncated\n",
    "\n",
    "      print(f\"Observation: {observation}, reward {reward}\")\n",
    "      print(env)\n",
    "\n",
    "    except:\n",
    "      done = True\n",
    "      print(\"Invalid action or KeyboardInterrupt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X910eylycXEf"
   },
   "source": [
    "We can define a function that performs `n_trajectories` rollouts on the environment with a given policy. We can also compute arbitrary statistics in the meanwhile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gU1XQ-ObDqCn"
   },
   "outputs": [],
   "source": [
    "def rollouts(env, policy, n_trajectories, gamma):\n",
    "    \"\"\"Execute policy over env for n_trajectories and compute discounted return.\"\"\"\n",
    "    total_return = 0.0\n",
    "\n",
    "    # Trajectores\n",
    "    for _ in range(n_trajectories):\n",
    "\n",
    "        # Init\n",
    "        discount = 1.0\n",
    "        ret = 0.0\n",
    "        observation, info = env.reset()\n",
    "        done = False\n",
    "\n",
    "        # Steps\n",
    "        while not done:\n",
    "\n",
    "            # Action selection\n",
    "            action = policy(observation)\n",
    "\n",
    "            # Transition\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            done = terminated or truncated\n",
    "            ret += reward * discount\n",
    "            discount *= gamma\n",
    "\n",
    "            if done:\n",
    "                total_return += ret\n",
    "\n",
    "    env.close()\n",
    "    return total_return / n_trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmYQPLghM8cy"
   },
   "source": [
    "Since we don't have a policy yet, let's define the uniform one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Itk9LgVqL-DD"
   },
   "outputs": [],
   "source": [
    "class UniformPolicy():\n",
    "    def __init__(self, n_actions: int):\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def __call__(self, observation):\n",
    "        return random.randint(0, self.n_actions-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4KDwEt06qnHI"
   },
   "outputs": [],
   "source": [
    "def make_policy(policy_dict):\n",
    "  \"\"\"Return the policy dictionary as a callable object\"\"\"\n",
    "\n",
    "  def callable_policy(observation):\n",
    "      return policy_dict[tuple(observation.tolist())]\n",
    "\n",
    "  return callable_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQmcdqKt31XT"
   },
   "source": [
    "Let's try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qXnOiHr234mn"
   },
   "outputs": [],
   "source": [
    "avg_return = rollouts(env=env,\n",
    "                      policy=UniformPolicy(env.action_space.n),\n",
    "                      n_trajectories=20,\n",
    "                      gamma=0.9)\n",
    "\n",
    "print(avg_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpv9bGrwdtFd"
   },
   "source": [
    "We can also visualize the execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k7_UY2RPOb7v"
   },
   "outputs": [],
   "source": [
    "from rlss_practice.wrappers import Renderer\n",
    "visible_env = Renderer(env)                # If colab\n",
    "#visible_env = Renderer(env, \"videos_dir\")  # if local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2CieU7A_40i"
   },
   "outputs": [],
   "source": [
    "avg_return = rollouts(env=visible_env,\n",
    "                      policy=UniformPolicy(env.action_space.n),\n",
    "                      n_trajectories=1,\n",
    "                      gamma=0.9)\n",
    "\n",
    "print(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbdR7lQQXhXu"
   },
   "outputs": [],
   "source": [
    "visible_env.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srQzHwZTe_gd"
   },
   "source": [
    "### Explicit model\n",
    "\n",
    "The algorithms we will implement require that a complete model of the environment is available, in the form of explicit transition and reward functions. This is not part of the gym interface.\n",
    "\n",
    "These two functions are stored in two members:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DczwaYuUfqHg"
   },
   "outputs": [],
   "source": [
    "# Transition and reward functions\n",
    "T = env.T\n",
    "R = env.R\n",
    "\n",
    "# These are represented as dictionaries (for maximum clarity)\n",
    "#   and indexed as T[state][action][next_state]\n",
    "print(\"A few probabilities\")\n",
    "print(T[(1, 1, 0)][2][(2, 1, 0)])\n",
    "print(T[(1, 1, 0)][2][(1, 1, 1)])\n",
    "print(T[(3, 1, 0)][1][(3, 1, 1)])\n",
    "\n",
    "#   also R[state][action]\n",
    "print(\"\\nA few rewards\")\n",
    "print(R[(1, 1, 0)][2])\n",
    "print(R[(3, 3, 0)][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQglr5MOiDCp"
   },
   "outputs": [],
   "source": [
    "# Explicit set of states and actions\n",
    "\n",
    "print(\"States\", env.states)\n",
    "print(\"Actions\", env.actions)\n",
    "print(\"Number of states\", len(env.states))\n",
    "print(\"Number of actions\", len(env.actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XfYfq9HebRHw"
   },
   "outputs": [],
   "source": [
    "# Some classic imports for the rest of the notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "\n",
    "mpl.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8L7zlYG8WcPq"
   },
   "source": [
    "## Solving the Minigrid task\n",
    "\n",
    "We would like to find a **stationary deterministic memoryless policy** or mapping from states to actions $\\pi : \\mathcal{S}\\rightarrow \\mathcal{A}$, that is able to arrive at the **goal state** sooner.\n",
    "\n",
    "In other words, our objective is to find $\\pi$ which maximizes the **expected discounted return** from any starting state represented by:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\rho(\\pi) &= \\mathbb{E}_{s_0\\sim\\nu_0, s_{t+1}\\sim p(\\cdot|s_t,\\pi(s_t))}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t,\\pi(s_t))\\right]\\\\\n",
    "    &= \\sum_{s}\\nu_0(s) V^{\\pi}(s)\n",
    "\\end{align*}\n",
    "\n",
    "with the **discount factor** $\\gamma\\in[0,1)$.\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "---\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Recall that the **state value function** for $s\\in \\mathcal{S}$ denoted\n",
    "\n",
    "\\begin{align*}\n",
    "    V^\\pi(s) &= \\mathbb{E}_{s_{t+1}\\sim p(\\cdot|s_t,\\pi(s_t))}\\left[\\sum_{t=0}^\\infty\\gamma^t r(s_t,\\pi(s_t))\\bigg|s_0=s\\right]\\\\\n",
    "    &=  \\sum_{a}\\pi(a|s)Q^{\\pi}(s,a)\n",
    "\\end{align*}\n",
    "\n",
    "represents the value of being in state $s$ and following policy $\\pi$ while the **action-value function** for $s\\in \\mathcal{S}, a\\in \\mathcal{A}$ denoted\n",
    "\n",
    "$$ Q^\\pi(s,a) = r(s,a) + \\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V^{\\pi}(s')$$\n",
    "\n",
    "is the value of first taking an action $a$ in state $s$ then following policy $\\pi$.\n",
    "\n",
    "By standard notation, the **Bellman operator** of policy $\\pi$ acting on functions $V : \\mathcal{S}\\rightarrow \\mathbb{R}$ (or vectors $V\\in\\mathbb{R}^{|\\mathcal{S}|}$) is given as:\n",
    "\n",
    "$$(T^{\\pi}V)(s) =  \\sum_{a}\\pi(a|s)\\bigg[r(s,a) + \\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V(s')\\bigg],\\quad s\\in\\mathcal{S}$$\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "---\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Ideally, we would like to find an **optimal policy** $\\pi^*$ with:\n",
    "\n",
    "$$ \\pi^*(s) \\in \\underset{a\\in\\mathcal{A}}{\\arg\\max} \\bigg\\{r(s,a) + \\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V^{\\pi^*}(s')\\bigg\\},\\quad s\\in\\mathcal{S}$$\n",
    "\n",
    "that maximizes our immediate reward and future return.\n",
    "\n",
    "The Bellman operator of $\\pi^*$ (a.k.a the **Bellman optimality operator**) acting on functions $V : \\mathcal{S}\\rightarrow \\mathbb{R}$ (or vectors $V\\in\\mathbb{R}^{|\\mathcal{S}|}$) is given as:\n",
    "\n",
    "$$(T^{*}V)(s) =  \\max_{a}\\bigg\\{r(s,a) + \\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V(s')\\bigg\\},\\quad s\\in\\mathcal{S}$$\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "---\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "With the transition and reward function of the grid-world task available to us, we attempt to find an optimal policy via **Dynamic programming**. Precisely, we implement **Policy Iteration** and **Value iteration** methods introduced in the first lecture.\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMWFCbNN7F8l"
   },
   "source": [
    "### Notation\n",
    "\n",
    "*   $r_{\\pi}\\in\\mathbb{R}^{|\\mathcal{S}|}$ so that for $s\\in\\mathcal{S}$, $r_{\\pi}(s) = r(s,\\pi(s))$.\n",
    "*   $p_{\\pi}\\in\\mathbb{R}^{|\\mathcal{S}|\\times|\\mathcal{S}|}$ so that for $s,s'\\in\\mathcal{S}$, $p_{\\pi}(s'|s) = p(s'|s,\\pi(s))$.\n",
    "*   $\\sum_{r}p(s',r|s,a)  = p(s'|s,a)r(s,a)$ since rewards are deterministic.\n",
    "*   $n_{k}$ is the number of loops required to compute $V_{k}$\n",
    "*   $\\Delta$ is a threshold on the accuracy of value estimation [[3]](#scrollTo=XqJPFe6f6gFV&line=5&uniqifier=1)\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nljnBe4bZTf"
   },
   "source": [
    "### Policy Iteration (PI) Recap\n",
    "\n",
    "**Idea**\n",
    "\n",
    "Gradually advance to $\\pi^{*}$ from an initial guess $\\pi_0$ through a series of **policy evaluation** and **policy improvement** steps.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Policy Evaluation step**\n",
    "\n",
    "Given a policy $\\pi_k$, compute $V^{\\pi_k}$ as\n",
    "\n",
    "* $V^{\\pi_k} = (I - \\gamma\\, p_{\\pi_k})^{-1}r_{\\pi_k}$\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Policy Improvement step**\n",
    "\n",
    "Obtain $\\pi_{k+1}$ as the greedy policy w.r.t $V^{\\pi_k}$ (or $V_{k}$). That is,\n",
    "$$ \\pi_{k+1}(s) \\in \\underset{a\\in\\mathcal{A}}{\\arg\\max} \\bigg\\{r(s,a) + \\gamma\\sum_{s'\\in\\mathcal{X}}p(s'|s,a)V^{\\pi_{k}}(s')\\bigg\\},\\quad s\\in\\mathcal{S}$$\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "#### Putting everything together\n",
    "\n",
    "Starting from an arbitrary stationary deterministic markovian policy $\\pi_{0}$, for $k = 0,1,2,\\cdots, K$ do:\n",
    "* Compute $V^{\\pi_k}$\n",
    "* Obtain $\\pi_{k+1}$ as the greedy policy w.r.t $V^{\\pi_k}$\n",
    "* Terminate loop if policy is stable (i.e $\\pi_{k+1}(s) = \\pi_{k}(s)$ for all $s\\in\\mathcal{S}$) and return $\\pi_{k+1}$.\n",
    "\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "#### Theoretical guarantee [[1,2]](#scrollTo=XqJPFe6f6gFV&line=5&uniqifier=1)\n",
    "\n",
    "PI finds an optimal policy after $K = \\mathcal{\\tilde{O}}(\\mathcal{SA}/(1-\\gamma))$ iterations.\n",
    "\n",
    "<!---\n",
    "Explicitly $K = \\mathcal{\\tilde{O}}((\\mathcal{SA - S})/(1-\\gamma))$ iterations.\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NFYLOHtPKwsG"
   },
   "outputs": [],
   "source": [
    "# Don't look at the solutions! This module contains the functions you should write\n",
    "from rlss_practice import DP_solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Woh49RAfbieB"
   },
   "outputs": [],
   "source": [
    "class PolicyIteration:\n",
    "  \"\"\"\n",
    "  Implements policy iteration\n",
    "  \"\"\"\n",
    "  def __init__(self, env: Env, gamma: float, initial_policy = None):\n",
    "    # Store\n",
    "    self.env = env\n",
    "    self.states = self.env.states\n",
    "    self.n_states = len(self.states)\n",
    "    self.actions = self.env.actions\n",
    "    self.n_actions = len(self.actions)\n",
    "    self.gamma = gamma\n",
    "    self.policy = initial_policy\n",
    "\n",
    "    # Default policy\n",
    "    if self.policy == None:\n",
    "      np.random.seed(4)\n",
    "      self.policy = {state: np.random.randint(0, self.n_actions-1) for state in self.states}\n",
    "\n",
    "    self.policy_stable = False\n",
    "    self.V = {state: 0.0 for state in env.states}\n",
    "    self.V_logs = []\n",
    "\n",
    "  def _evaluate_policy(self):\n",
    "    \"\"\"\n",
    "    Given 'policy' π_{k} compute V^{π_{k}}. Let,\n",
    "\n",
    "      A = (I - \\gamma p_{π_{k}})\n",
    "      b = r_{π_{k}}\n",
    "      x = V^{π_{k}}\n",
    "\n",
    "    solve the system of linear equations Ax=b\n",
    "\n",
    "    :return x: a |S|x1 array\n",
    "    \"\"\"\n",
    "    # Task: complete this and return x instead of solutions...\n",
    "    # TODO: Compute A\n",
    "\n",
    "    # TODO: Compute b\n",
    "\n",
    "    # TODO: solve for x\n",
    "\n",
    "    return DP_solutions.PolicyIteration._evaluate_policy(self)\n",
    "\n",
    "\n",
    "  def evaluate_policy(self):\n",
    "    \"\"\"\n",
    "    Collect state values in dict\n",
    "    \"\"\"\n",
    "    V_array = self._evaluate_policy()\n",
    "\n",
    "    # Assign values to holder as dictionary\n",
    "    self.V = {state: V_array[i].item() for i, state in enumerate(self.states)}\n",
    "\n",
    "    # Append values to log for plots\n",
    "    self.V_logs.append(self.V.copy())\n",
    "\n",
    "\n",
    "  def get_policy(self):\n",
    "    \"\"\"\n",
    "    Get the greedy policy:\n",
    "\n",
    "      π_{k+1}(s) = argmax_{a\\in A} Q^{\\pi_{k}}(s,a)\n",
    "\n",
    "    where\n",
    "\n",
    "      Q^{\\pi_{k}}(s,a) = r(s,a) + gamma * <P(.|s,a),V^{\\pi_{k}}>\n",
    "\n",
    "    assign new policy to self.policy\n",
    "    update self.policy_stable\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Solve this and delete the next line\n",
    "    DP_solutions.PolicyIteration.get_policy(self)\n",
    "\n",
    "\n",
    "  def get_expected_update(self, state, action):\n",
    "    \"\"\"\n",
    "    Compute:\n",
    "\n",
    "      Q(s,a) = r(s,a) + gamma <p(.|s,a),V>\n",
    "\n",
    "    :param state\n",
    "    :param action\n",
    "\n",
    "    :return Q(s,a): float\n",
    "    \"\"\"\n",
    "    # TODO: Solve this and return Q(s,a) instead\n",
    "    return DP_solutions.PolicyIteration.get_expected_update(self, state, action)\n",
    "\n",
    "\n",
    "  def get_p_pi(self):\n",
    "    \"\"\"\n",
    "    Given π_{k}, compute p_{π_{k}}\n",
    "    \"\"\"\n",
    "    p_pi = np.zeros((self.n_states,self.n_states))\n",
    "\n",
    "    for s_index, s in enumerate(self.states):\n",
    "      for snext_index, snext in enumerate(self.states):\n",
    "        p_pi[s_index, snext_index] = self.env.T[s][self.policy[s]][snext]\n",
    "\n",
    "    return p_pi\n",
    "\n",
    "\n",
    "  def get_r_pi(self):\n",
    "    \"\"\"\n",
    "    Given π_{k}, compute r_{π_{k}}\n",
    "    \"\"\"\n",
    "    r_pi = np.zeros((self.n_states,1))\n",
    "\n",
    "    for i, state in enumerate(self.states):\n",
    "      r_pi[i][0] = self.env.R[state][self.policy[state]]\n",
    "\n",
    "    return r_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pTZRo-jxBxQ"
   },
   "source": [
    "Let's try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKL72UxrxFJl"
   },
   "outputs": [],
   "source": [
    "# Run policy iteration\n",
    "PI_planner1 = PolicyIteration(env,\n",
    "                              gamma=0.9)\n",
    "\n",
    "while not PI_planner1.policy_stable:\n",
    "  PI_planner1.evaluate_policy()\n",
    "  PI_planner1.get_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQkER_MsJiCK"
   },
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6btXwfrRJev2"
   },
   "outputs": [],
   "source": [
    "def visualize(vlogs, state, grid_size):\n",
    "  grid_size -= 2   # walls\n",
    "\n",
    "  # Initial state values\n",
    "  fig1 = plt.figure(figsize=(4, 3))\n",
    "  ax1 = fig1.subplots()\n",
    "  ax1.set_ylabel(\"value of (1,1,0)\")\n",
    "  ax1.set_xlabel(\"iterations\")\n",
    "  initial_state_values = [value[(1, 1, 0)] for value in PI_planner1.V_logs]\n",
    "  ax1.plot(range(len(initial_state_values)), initial_state_values)\n",
    "\n",
    "  # States values over time\n",
    "  some_values_over_time = [\n",
    "    {(x, y): values[(x, y, o)] for (x, y, o) in env.states if o == 0}  # o is a fixed agent orientation\n",
    "    for values in vlogs\n",
    "  ]\n",
    "  values_over_time = [\n",
    "    np.array([[values[(x+1, y+1)] for x in range(grid_size)] for y in range(grid_size)])\n",
    "    for values in some_values_over_time\n",
    "  ]\n",
    "  vmids = [(values.max() + values.min()) / 2 for values in values_over_time]\n",
    "  vmin = min([values.min() for values in values_over_time])\n",
    "  vmax = max([values.max() for values in values_over_time])\n",
    "\n",
    "  steps = len(values_over_time)\n",
    "  fig2 = plt.figure(figsize=(4 * steps, 3))\n",
    "  axs2 = fig2.subplots(1, steps)\n",
    "  if isinstance(axs2, plt.Axes):\n",
    "    axs2 = [axs2]\n",
    "    multistep = False\n",
    "  else:\n",
    "    multistep = True\n",
    "\n",
    "  for i, ax in enumerate(axs2):\n",
    "    ax.set_title(f\"values at step {i}\" if multistep else \"values\")\n",
    "    ax.imshow(values_over_time[i], cmap=\"Blues\", vmin=vmin, vmax=vmax)\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.yaxis.set_visible(False)\n",
    "    for x in range(grid_size):\n",
    "      for y in range(grid_size):\n",
    "        val = values_over_time[i][y,x]\n",
    "        ax.text(x, y, f\"{val:.2f}\", ha=\"center\", va=\"center\", color=\"w\" if val > vmids[i] else \"k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XvCmq5bQQqEi"
   },
   "outputs": [],
   "source": [
    "visualize(PI_planner1.V_logs, (1, 1, 0), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meYQb9VOTnDT"
   },
   "source": [
    "We can also test the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3C-WmCiVT6op"
   },
   "outputs": [],
   "source": [
    "avg_return = rollouts(env=visible_env,\n",
    "                      policy=make_policy(PI_planner1.policy),\n",
    "                      n_trajectories=20,\n",
    "                      gamma=0.9)\n",
    "\n",
    "print(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGxMz5yZEZ-N"
   },
   "outputs": [],
   "source": [
    "visible_env.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymYjjFBYbiy-"
   },
   "source": [
    "What if we have failure probabilities now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8EcH0mS2bt1N"
   },
   "outputs": [],
   "source": [
    "# Define\n",
    "env = Room(\n",
    "    failure=0.2,\n",
    "    agent_start_pos=(1, 1),\n",
    "    agent_start_dir=1,\n",
    "    size=6,\n",
    ")\n",
    "visible_env = Renderer(env)\n",
    "print(env)\n",
    "\n",
    "# Plan\n",
    "PI_planner1 = PolicyIteration(env,\n",
    "                              gamma=0.9)\n",
    "\n",
    "while not PI_planner1.policy_stable:\n",
    "  PI_planner1.evaluate_policy()\n",
    "  PI_planner1.get_policy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gvuqeY88cCW2"
   },
   "outputs": [],
   "source": [
    "# Visualize\n",
    "avg_return = rollouts(\n",
    "    env=visible_env,\n",
    "    policy=make_policy(PI_planner1.policy),\n",
    "    n_trajectories=20,\n",
    "    gamma=0.9\n",
    ")\n",
    "print(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-hHn_KXcKaS"
   },
   "outputs": [],
   "source": [
    "visible_env.play()\n",
    "\n",
    "visualize(PI_planner1.V_logs, (1, 1, 1), 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzJ9fQ8ceZIB"
   },
   "source": [
    "But matrix inversions can be quite expensive. Consider iterative policy evaluation instead.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "### Modified Policy Iteration (MPI) Recap\n",
    "\n",
    "**Idea**\n",
    "\n",
    "Same as PI but with **truncated policy evaluation**\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Truncated Policy Evaluation step**\n",
    "\n",
    "Given a policy $\\pi_k$, estimate $V^{\\pi_k}$ as $V_{k}$ with the following iterations\n",
    "\n",
    "  * Initialize $V$ as $\\mathbf{0}$ when $k = 0$ and $V_{k-1}$ otherwise, let $\\pi = \\pi_k$\n",
    "\n",
    "    <div>\n",
    "    <img src=\"https://drive.google.com/uc?export=view&id=1QaMg7a6HELjYycAnm6RE9vnzHD_fnaCn\" alt=\"iterative policy evaluation\" width=\"600\"/>\n",
    "    </div>\n",
    "\n",
    "    Return $V_k = V$. <!--$(T^{\\pi_{k}})^{n_{k}}V_{k-1}$-->\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "#### Putting everything together\n",
    "\n",
    "Starting from an arbitrary stationary deterministic markovian policy $\\pi_{0}$, for $k = 0,1,2,\\cdots, K$ do:\n",
    "* Estimate $V^{\\pi_k}$ with $V_{k}=(T^{\\pi_{k}})^{n_{k}}V_{k-1}$\n",
    "* Obtain $\\pi_{k+1}$ as the greedy policy w.r.t $V^{\\pi_k}$\n",
    "* Terminate loop if policy is stable (i.e $\\pi_{k+1}(s) = \\pi_{k}(s)$ for all $s\\in\\mathcal{S}$) and return $\\pi_{k+1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nFkMVMI3syvt"
   },
   "outputs": [],
   "source": [
    "class ModifiedPolicyIteration(PolicyIteration):\n",
    "  \"\"\"\n",
    "  Implements policy iteration with truncated policy evaluation\n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "              env: Env,\n",
    "              gamma: float,\n",
    "              theta: float,\n",
    "              initial_policy = None):\n",
    "\n",
    "    super().__init__(env, gamma, initial_policy)\n",
    "    self.theta = theta\n",
    "    self.V = {state: 0.0 for state in env.states}\n",
    "\n",
    "\n",
    "  def evaluate_policy(self):\n",
    "    \"\"\"\n",
    "    Given 'policy' π_{k},\n",
    "    Starting from a previous guess 'V_{k-1}',\n",
    "    iteratively estimate V^{π_{k}} as T^{π_{k}}V_{k-1}.\n",
    "\n",
    "    assign new state values to self.V\n",
    "    \"\"\"\n",
    "    # Task: Complete this and delete line *\n",
    "\n",
    "    # Set Delta\n",
    "    Delta = np.inf\n",
    "\n",
    "    # Main loop for iterative value update\n",
    "    while Delta > self.theta:\n",
    "      # Initialize Delta\n",
    "      Delta = 0\n",
    "\n",
    "      # ToDo: Loop over states\n",
    "\n",
    "        #ToDo: Update the state value\n",
    "\n",
    "        #ToDo: Update Delta\n",
    "\n",
    "    DP_solutions.ModifiedPolicyIteration._evaluate_policy(self)    #*\n",
    "\n",
    "    self.V_logs.append(self.V.copy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KofhGOYGhckf"
   },
   "source": [
    "Now we run Modified policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9uBUwD-hhSF"
   },
   "outputs": [],
   "source": [
    "MPI_planner = ModifiedPolicyIteration(env,\n",
    "                                      gamma=0.9, #Same as PI\n",
    "                                      theta = 0.1)\n",
    "\n",
    "while not MPI_planner.policy_stable:\n",
    "  MPI_planner.evaluate_policy()\n",
    "  MPI_planner.get_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70ieiSAZqR6H"
   },
   "outputs": [],
   "source": [
    "avg_return = rollouts(env=visible_env,\n",
    "                      policy=make_policy(MPI_planner.policy),\n",
    "                      n_trajectories=10,\n",
    "                      gamma=0.9)\n",
    "\n",
    "print(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCueLRiiqvaQ"
   },
   "outputs": [],
   "source": [
    "visible_env.play()\n",
    "\n",
    "visualize(MPI_planner.V_logs, (1, 1, 1), 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IA8AkX2Feuwt"
   },
   "source": [
    "### Value Iteration (VI) Recap\n",
    "\n",
    "**Idea**\n",
    "\n",
    "Gradually advance to $\\pi^{*}$ with combined **truncated policy evaluation** and **policy improvement** steps.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Truncated Policy Evaluation and Improvement step**\n",
    "\n",
    "No arbitrary starting policy needed. Estimate $V^{\\pi^*}$ as follows:\n",
    "\n",
    "  * For $k = 0,1,2,\\cdots, K$\n",
    "\n",
    "    * Set $V = \\mathbf{0}$ when $k = 0$ and $V_{k-1}$ otherwise\n",
    "      <div>\n",
    "      <img src=\"https://drive.google.com/uc?export=view&id=1fsi3ZZgqZ-p061AxvSdeluWZTlwSjnGj\" alt=\"policy evaluation and improvement\" width=\"400\"/>\n",
    "      </div>\n",
    "\n",
    "      return $V$ as $V_{k}$.\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "#### Putting everything together\n",
    "\n",
    "* Estimate $V^{\\pi^*}$ with $V_{K} = (T^*)^{K}\\mathbf{0}$\n",
    "* Return $\\hat{\\pi}^{*}$ as the greedy policy w.r.t $V_{K}$. That is,\n",
    "\n",
    "  $$ \\hat{\\pi}^{*}(s) \\in \\underset{a\\in\\mathcal{A}}{\\arg\\max} \\bigg\\{r(s,a) + \\gamma\\sum_{s'\\in\\mathcal{X}}p(s'|s,a)V_{K}(s')\\bigg\\},\\quad s\\in\\mathcal{S}$$\n",
    "\n",
    "\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "#### Theoretical guarantee [[2]](#scrollTo=XqJPFe6f6gFV&line=5&uniqifier=1)\n",
    "\n",
    "VI finds an **$\\mathbf{\\varepsilon}$-optimal policy** ($\\pi^{\\varepsilon}$) satisfying\n",
    "\n",
    "$\\qquad V^* - V^{\\pi^{\\varepsilon}}\\leq \\varepsilon\\,\\mathbf{1}$\n",
    "\n",
    "after $K = \\mathcal{O}(\\ln(2\\gamma/\\varepsilon(1-\\gamma)^2)/(1-\\gamma))$ iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cWtqSTiD9Kko"
   },
   "outputs": [],
   "source": [
    "class ValueIteration:\n",
    "  \"\"\"\n",
    "  Implements value iteration\n",
    "  \"\"\"\n",
    "  def __init__(self,\n",
    "              env: Env,\n",
    "              gamma: float,\n",
    "              epsilon: float,\n",
    "              num_iterations = None,\n",
    "  ):\n",
    "    self.env = env\n",
    "    self.states = self.env.states\n",
    "    self.n_states = len(self.states)\n",
    "    self.actions = self.env.actions\n",
    "    self.n_actions = len(self.actions)\n",
    "    self.gamma = gamma\n",
    "    self.K = num_iterations\n",
    "    self.V_logs = []\n",
    "\n",
    "    if self.K is None:\n",
    "      self.K = math.ceil(np.log((2*self.gamma)/(epsilon*(1-self.gamma)**2))/(1-self.gamma))\n",
    "\n",
    "    self.policy = {state: 0 for state in self.env.states}\n",
    "    self.V = {state: 0 for state in self.env.states}\n",
    "\n",
    "\n",
    "  def estimate_vstar(self):\n",
    "    \"\"\"\n",
    "    Starting from an initial guess 'V',\n",
    "    iteratively estimate V^{π*} as T*V\n",
    "\n",
    "    Assign new values to self.V\n",
    "    \"\"\"\n",
    "    # Task: Complete this and delete line *\n",
    "\n",
    "    # Main loop for iterative value update\n",
    "    for k in range(self.K):\n",
    "\n",
    "      # ToDo: Loop over states\n",
    "\n",
    "        #ToDo: Update the state value\n",
    "      pass\n",
    "\n",
    "\n",
    "    DP_solutions.ValueIteration._estimate_vstar(self)    #*\n",
    "\n",
    "    self.V_logs.append(self.V.copy())\n",
    "\n",
    "\n",
    "  def get_policy(self):\n",
    "    \"\"\"\n",
    "    Get the greedy policy:\n",
    "      π(s) = argmax_{a\\in A} Q(s,a)\n",
    "    where\n",
    "      Q(s,a) = r(s,a) + gamma*<P(.|s,a),v>\n",
    "\n",
    "    assign new policy to self.policy\n",
    "    \"\"\"\n",
    "    # TODO: Solve this and delete the next line\n",
    "    return PolicyIteration.get_policy(self)\n",
    "\n",
    "\n",
    "  def get_expected_update(self, state, action):\n",
    "    \"\"\"\n",
    "    Compute:\n",
    "\n",
    "      Q(s,a) = r(s,a) + gamma <p(.|s,a),V>\n",
    "\n",
    "    :param state\n",
    "    :param action\n",
    "\n",
    "    :return Q(s,a): float\n",
    "    \"\"\"\n",
    "    # TODO: Same as for PI. Solve this and return Q(s,a) instead\n",
    "    return PolicyIteration.get_expected_update(self, state, action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7DXuWm3v82W"
   },
   "source": [
    "Finally, we also plan with Value Iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BFFnzxCfaS-E"
   },
   "outputs": [],
   "source": [
    "VI_planner = ValueIteration(env,\n",
    "                            gamma = 0.9, #same as PI\n",
    "                            epsilon = 0.01,\n",
    "                            num_iterations = None) #You can set the number of iterations\n",
    "\n",
    "VI_planner.estimate_vstar()\n",
    "VI_planner.get_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dgj9qrYohLWm"
   },
   "outputs": [],
   "source": [
    "avg_return = rollouts(env=visible_env,\n",
    "                      policy=make_policy(VI_planner.policy),\n",
    "                      n_trajectories=20,\n",
    "                      gamma=0.9)\n",
    "\n",
    "print(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QY-Vrp5J0gDC"
   },
   "outputs": [],
   "source": [
    "visible_env.play()\n",
    "\n",
    "visualize(VI_planner.V_logs, (1, 1, 0), 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fS7zGk2tas0"
   },
   "source": [
    "### Bonus Task ⭐\n",
    "\n",
    "Implement Optimistic Policy Iteration. See slide 33 of [[1]](#scrollTo=XqJPFe6f6gFV&line=5&uniqifier=1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqJPFe6f6gFV"
   },
   "source": [
    "# Credit:\n",
    "\n",
    "1.   Bruno Scherrer, \"a lecture on Markov Decision Processes and Dynamic Programming\", June 2023, [Slides](https://drive.google.com/file/d/1sFh0TyU_nq60R7kouPg6MCI4IMYicSII/view?usp=sharing)\n",
    "2.   Csaba Szepesvári \"a lecture series on Theoretical Foundations of Reinforcement Learning\", 2020, [RL Theory course](https://rltheory.github.io/)\n",
    "3.   Richard S. Sutton, Andrew G. Barto \"Reinforcement Learning: An Introduction\", second edition, 2020, [Book](http://incompleteideas.net/book/RLbook2020.pdf)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
