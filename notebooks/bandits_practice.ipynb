{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rlsummerschool/practical-sessions/blob/master/notebooks/bandits_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jq49Umhtn6Ws",
      "metadata": {
        "id": "jq49Umhtn6Ws"
      },
      "source": [
        "# Bandit Algorithms\n",
        "\n",
        "This practical session of [RLSS 2023](https://rlsummerschool.com/) is based on the lectures:\n",
        "* Stochastic Multi-Armed Bandits by Emilie Kaufmann ([slides](https://drive.google.com/file/d/1CXrOd8Ltc5x9QmAGmxNWaYmWgZY5V2kJ/view?usp=sharing))\n",
        "* Contextual Bandits by Claire Vernade ([slides](https://drive.google.com/file/d/11G6_mvo9DY1nQR3qsZ6X__3TpPTcBFXH/view?usp=sharing))\n",
        "\n",
        "It was developed by [Julia Olkhovskaya](https://sites.google.com/view/julia-olkhovskaya/home) and Matteo Papini building upon notebooks by Claire Vernade."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad962caa",
      "metadata": {
        "id": "ad962caa"
      },
      "source": [
        "As a motivation for this tutorial, we consider the multiple choice testing for the webpage design problem.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "<div>\n",
        "    <img src=\"https://github.com/rlsummerschool/practical-sessions/blob/master/images/website.jpg?raw=true\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "<br><br>\n",
        "\n",
        "Good design of a webpage is not only about the matching colours and useful\n",
        "interfaces. Even if one website\n",
        "design seems to be the most rational for the designer, it is usually hard to\n",
        "predict which webpage design users will like the most. Therefore, the\n",
        "important part of the webpage design is the testing on the real users.\n",
        "Since displaying the webpage with bad design means the loss of the money,\n",
        "we need to develop a mecanism that chooses the page design that is\n",
        "the best most of time.\n",
        "\n",
        "*Your goal is to build a decision support system which chooses the website design at each time step without any prior knowledge.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4vGsiCOKpBjC",
      "metadata": {
        "id": "4vGsiCOKpBjC"
      },
      "outputs": [],
      "source": [
        "# Set up the environment\n",
        "!pip install \"git+https://github.com/rlsummerschool/practical-sessions.git\" --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5983e3b1",
      "metadata": {
        "id": "5983e3b1"
      },
      "outputs": [],
      "source": [
        "# Some standard imports\n",
        "import numpy as np\n",
        "from scipy.stats import bernoulli\n",
        "from math import log\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "matplotlib.style.use('seaborn-v0_8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d287e889",
      "metadata": {
        "id": "d287e889"
      },
      "outputs": [],
      "source": [
        "# Some imports from our github repository (https://github.com/rlsummerschool/practical-sessions/tree/master)\n",
        "from rlss_practice.model import Environment, Agent, MAB_env\n",
        "import rlss_practice.model\n",
        "from rlss_practice.display import plot_result\n",
        "import rlss_practice.bandit_solutions as solutions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JL_AymqzsgM6",
      "metadata": {
        "id": "JL_AymqzsgM6"
      },
      "source": [
        "# Part 1 &mdash; Multi-Armed Bandits\n",
        "\n",
        "We formulate the choice design of a webpage design as a multi-armed bandit problem.  We\n",
        "will use the synthetically generated ‚Äúvisits‚Äù of users, for each user the system will recommend the version of the website, so each version of the website is an arm. We model the engagement score (~#clicks) corresponding to the of version of the website as a Gaussian random variable."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bdb534e",
      "metadata": {
        "id": "7bdb534e"
      },
      "source": [
        "## Interaction protocol\n",
        "Let's recall Emilie's lecture from yesterday. An (adaptive) agent interacts with an unknown environment for $T$ rounds. At each round $t$:\n",
        "\n",
        "* The agent picks an arm (action) $A_t$ from a set of $K$ actions\n",
        "* The agent receives a reward $R_t \\sim \\nu_{A_t}$\n",
        "\n",
        "where $\\nu_{a}$ is an *unknown* distribution with mean $\\mu_a$. In our case, it's a Gaussian distribution *with standard deviation $\\sigma=1$ for all arms*:\n",
        "\n",
        "$R_t \\sim N(\\mu_a; 1)$\n",
        "\n",
        "The goal is to maximize total reward $\\sum_{t=1}^TR_t$.\n",
        "\n",
        "Here is some code to play a bandit algorithm (adaptive agent) in a MAB environment, and to compare the performance on different algorithms on the same problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8c68647",
      "metadata": {
        "id": "b8c68647"
      },
      "outputs": [],
      "source": [
        "def play_mab(environment, agent, N, T):\n",
        "    \"\"\"\n",
        "    Play N independent runs of length T for the specified agent.\n",
        "\n",
        "    :param environment: a MAB instance\n",
        "    :param agent: a bandit algorithm\n",
        "    :param N: number of independent simulations\n",
        "    :param T: decision horizon\n",
        "    :return: the agent's name, and the collected data in numpy arrays\n",
        "    \"\"\"\n",
        "\n",
        "    rewards = np.zeros((N, T))\n",
        "    regrets = np.zeros((N, T))\n",
        "    pseudo_regrets = np.zeros((N, T))\n",
        "    avg_rewards = np.zeros((N, T))\n",
        "\n",
        "    for n in range(N):\n",
        "        agent.reset()\n",
        "        for t in range(T):\n",
        "            action = agent.get_action()\n",
        "            reward = environment.get_reward(action)\n",
        "            agent.receive_reward(action,reward)\n",
        "\n",
        "            # compute instantaneous reward  and (pseudo) regret\n",
        "            rewards[n,t] = reward\n",
        "            means = environment.get_means()\n",
        "            best_reward = np.max(means)\n",
        "            regrets[n,t]= best_reward - reward # this can be negative due to the noise, but on average it's positive\n",
        "            avg_rewards[n,t] = means[action]\n",
        "            pseudo_regrets[n,t] = best_reward - means[action]\n",
        "\n",
        "    return agent.name(), rewards, regrets, avg_rewards, pseudo_regrets\n",
        "\n",
        "\n",
        "def experiment_mab(environment, agents, N, T, mode=\"regret\"):\n",
        "    \"\"\"\n",
        "    Play N trajectories for all agents over a horizon T. Store data in a dictionary.\n",
        "\n",
        "    :param environment: a MAB instance\n",
        "    :param agent: a list of bandit algorithms to compare\n",
        "    :param N: number of independent simulations\n",
        "    :param T: decision horizon\n",
        "    :param mode: the performance measure to return (\"reward\", \"average reward\", \"regret\", \"pseudo regret\")\n",
        "    :return: the performance for each agent in a dictionary indexed by the agent's name\n",
        "    \"\"\"\n",
        "\n",
        "    all_data = {}\n",
        "\n",
        "    for agent in agents:\n",
        "        agent_id, rewards, regrets, avg_rewards, pseudo_regrets = play_mab(environment, agent, N, T)\n",
        "\n",
        "        if mode == \"regret\":\n",
        "            all_data[agent_id] = regrets\n",
        "        elif mode == \"pseudo regret\":\n",
        "            all_data[agent_id] = pseudo_regrets\n",
        "        elif mode == \"reward\":\n",
        "            all_data[agent_id] = rewards\n",
        "        elif mode == \"average reward\":\n",
        "            all_data[agent_id] = avg_rewards\n",
        "        else:\n",
        "            raise ValueError\n",
        "\n",
        "    return all_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae4f9f27",
      "metadata": {
        "id": "ae4f9f27"
      },
      "source": [
        "Our MAB environment has 3 arms (three possible website designs). We will run online algorithms for $T=1000$ steps (corresponding to 1000 users visiting the website in sequence) and average results over $N=50$ independent runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae536f65",
      "metadata": {
        "id": "ae536f65"
      },
      "outputs": [],
      "source": [
        "K = 3  # number of arms (website versions)\n",
        "\n",
        "env = MAB_env(solutions.MEANS) #We don't know the reward distributions in advance!\n",
        "\n",
        "T = 1000  # Horizon\n",
        "N = 50  # number of simulations\n",
        "\n",
        "# Visualization\n",
        "Nsub = 100 # Subsampled points\n",
        "tsav = range(2, T, Nsub)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14e6ed86",
      "metadata": {
        "id": "14e6ed86"
      },
      "source": [
        "## Exercise 1.A &mdash; (Epsilon) Greedy\n",
        "**tl;dr**\n",
        "> $A_t = \\arg\\max_{a\\in\\{1,\\dots,K\\}} \\widehat{\\mu}_a(t-1)$ with probability $1-\\epsilon$\n",
        ">\n",
        "> $A_t \\sim \\mathrm{Uniform}(1,\\dots,K)$ otherwise\n",
        "\n",
        "Let's start with the $\\epsilon$-greedy algorithm, where $\\epsilon$ is an \"exploration\" parameter.\n",
        "\n",
        "üìñ The way $\\epsilon$-greedy selects actions is\n",
        "\n",
        "$A_t = \\arg\\max_{a\\in\\{1,\\dots,K\\}} \\widehat{\\mu}_a(t-1)$ with probability $1-\\epsilon$\n",
        "\n",
        "$A_t \\sim \\mathrm{Uniform}(1,\\dots,K)$ otherwise\n",
        "\n",
        "Where $\\widehat{\\mu}_a(t-1)$ is the average reward for arm $a$ up to time $t-1$ included.\n",
        "\n",
        "You are given an *incomplete* implementation of $\\epsilon$-greedy that actually implements the Greedy algorithm ($\\epsilon=0$). This code already includes computation of average rewards!\n",
        "\n",
        "Your task is to:\n",
        "* Complete the implementation of $\\epsilon$-greedy [(hint)](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html)\n",
        "* Find a good value of $\\epsilon$ for our 3-arms problem\n",
        "* Compare the regret with Greedy ($\\epsilon=0$)\n",
        "* üí™ **Bonus question:** Implement a version of $\\epsilon$-greedy where the exploration parameter changes with time, as seen in Emilie's lecture. Does it work better?\n",
        "\n",
        "üéÅ For all (non-bonus) questions you can try our **solution** by accessing the classes in `solutions`. For instance, our implementation of EpsilonGreedy is `solution.EpsilonGreedy`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23c6a12d",
      "metadata": {
        "id": "23c6a12d"
      },
      "outputs": [],
      "source": [
        "class EpsilonGreedy(Agent):\n",
        "  \"\"\"\n",
        "  Epsilon-Greedy MAB algorithm\n",
        "\n",
        "  :param K: number of arms\n",
        "  :param eps: exploration probability\n",
        "  \"\"\"\n",
        "  def __init__(self, K, eps=0.):\n",
        "    self.eps = eps\n",
        "    self.K = K\n",
        "    self.reset()\n",
        "\n",
        "    self.cumulative_reward = np.zeros(self.K)\n",
        "    self.num_played = np.zeros(self.K)\n",
        "    self.avg_rewards = np.zeros(self.K)\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Reset history before each independent run\"\"\"\n",
        "    self.t = 0\n",
        "    self.avg_rewards = np.zeros(self.K)\n",
        "    self.cumulative_reward = np.zeros(self.K)\n",
        "    self.num_played = np.zeros(self.K)\n",
        "\n",
        "  def get_action(self):\n",
        "    \"\"\"\n",
        "    Select the arm to play based on current history\n",
        "\n",
        "    :return: the selected arm (an integer between 0 and K-1)\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE:\n",
        "    # TODO: Add some instructions to turn Greedy into epsilon-Greedy\n",
        "\n",
        "    ...\n",
        "\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "    chosen_arm_index = np.argmax(self.avg_rewards)\n",
        "    return chosen_arm_index\n",
        "\n",
        "  def receive_reward(self, chosen_arm, reward):\n",
        "    \"\"\"\n",
        "    Update history after playing an arm (counts and average rewards)\n",
        "\n",
        "    :param chosen_arm: the arm that was played\n",
        "    :param reward: the reward that was received\n",
        "    \"\"\"\n",
        "    self.cumulative_reward[chosen_arm] += reward\n",
        "    self.num_played[chosen_arm] += 1\n",
        "    self.avg_rewards[chosen_arm] = self.cumulative_reward[chosen_arm]/self.num_played[chosen_arm] # update\n",
        "\n",
        "    self.t += 1\n",
        "\n",
        "  def name(self):\n",
        "    return 'EGreedy('+str(self.eps)+')'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9GeRPShzfZak",
      "metadata": {
        "id": "9GeRPShzfZak"
      },
      "source": [
        "First let's see how we can visualize the result of an algorithm, using Greedy as an example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e33338c",
      "metadata": {
        "id": "9e33338c"
      },
      "outputs": [],
      "source": [
        "greedy = EpsilonGreedy(K,  eps=0.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7s9Udc3Cf90w",
      "metadata": {
        "id": "7s9Udc3Cf90w"
      },
      "source": [
        "ü§ì What we can measure **in a real bandit application** is just the rewards of the selected arms! This is the nature of bandit feedback.\n",
        "\n",
        "Results are averaged over the N runs. In a real online application, there is *only one run!*\n",
        "\n",
        "The `q` parameter of `plot_result` specifies the quantile of the error bars (shaded area)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae683756",
      "metadata": {
        "id": "ae683756"
      },
      "outputs": [],
      "source": [
        "greedy_experiment = experiment_mab(env, [greedy], N=N, T=T, mode=\"reward\")\n",
        "\n",
        "plot_result(greedy_experiment, q=10, mode=\"reward\", cumulative=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cjh1QowgMn4",
      "metadata": {
        "id": "7cjh1QowgMn4"
      },
      "source": [
        "For better visualization, we can also sum rewards over time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f45b1f4",
      "metadata": {
        "id": "3f45b1f4"
      },
      "outputs": [],
      "source": [
        "greedy_experiment = experiment_mab(env, [greedy], N=N, T=T, mode=\"reward\")\n",
        "plot_result(greedy_experiment, q=10, mode=\"reward\", cumulative=True);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NbmwLquWgLF9",
      "metadata": {
        "id": "NbmwLquWgLF9"
      },
      "source": [
        "In research experiments, we often test algorithms on known distribution. This allows us to compute the (cumulative) regret:\n",
        "\n",
        "$\\mathrm{Regret}(T) = T\\mu_\\star - \\sum_{t=1}^TR_t$\n",
        "\n",
        "Of course the agent doesn't have acces to this information, neither would we in a real application.\n",
        "\n",
        "‚ö†Ô∏è Remember that **smaller regret is always better** when comparing algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a10d601",
      "metadata": {
        "id": "7a10d601"
      },
      "outputs": [],
      "source": [
        "greedy_experiment = experiment_mab(env, [greedy], N=N, T=T, mode=\"regret\")\n",
        "plot_result(greedy_experiment, q=10, mode=\"regret\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdxWkGt-_-j-",
      "metadata": {
        "id": "fdxWkGt-_-j-"
      },
      "source": [
        "Finally, we can ignore the noise in the rewards and compute the **pseudo regret** instead (again, only in a controlled experiment):\n",
        "\n",
        "$\\widetilde{\\mathrm{Regret}}(T) = T\\mu_\\star - \\sum_{t=1}^T\\mu_{A_t}$\n",
        "\n",
        "We will use pseudo regret going on.\n",
        "\n",
        "ü§ì Note that it is still a random quantity! (can you see why?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qx8rTXqB_3WL",
      "metadata": {
        "id": "qx8rTXqB_3WL"
      },
      "outputs": [],
      "source": [
        "greedy_experiment = experiment_mab(env, [greedy], N=N, T=T, mode=\"pseudo regret\")\n",
        "plot_result(greedy_experiment, q=10, mode=\"pseudo regret\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ubVt2OOjBCsi",
      "metadata": {
        "id": "ubVt2OOjBCsi"
      },
      "source": [
        "üöÄ Now it's time to test you implementation of epsilon greedy!\n",
        "\n",
        "ü§ì Remember: whatever bandit algorithm you use in your application, always make sure to compare it with Greedy first. Sometimes, Greedy (supervised learning) is all your client/boss will be willing to deploy..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O-IUwWk6AlNh",
      "metadata": {
        "id": "O-IUwWk6AlNh"
      },
      "outputs": [],
      "source": [
        "eps_greedy = EpsilonGreedy(K,  eps=...) # Pick your epsilon!\n",
        "\n",
        "eps_greedy_experiment = experiment_mab(env, [greedy, eps_greedy], N=N, T=T, mode=\"pseudo regret\")\n",
        "plot_result(eps_greedy_experiment, q=10, mode=\"pseudo regret\");"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ü§ì Did you find a good value of $\\epsilon$? Well, in a real *online* application you have only one trial, so you can't really do hyper-parameter tuning..."
      ],
      "metadata": {
        "id": "JSQytANFuuCH"
      },
      "id": "JSQytANFuuCH"
    },
    {
      "cell_type": "markdown",
      "id": "2229aeb4",
      "metadata": {
        "id": "2229aeb4"
      },
      "source": [
        "## Exercise 1.B &mdash; UCB\n",
        "\n",
        "**tl;dr**\n",
        "> $A_t = \\arg\\max_{a\\in\\{1,\\dots,K\\}} \\left\\{\\widehat{\\mu}_a(t) + \\sqrt{\\frac{\\alpha \\log t}{N_a(t)}}\\right\\}$\n",
        "\n",
        "üîß Now you will have to complete the implementation of UCB:\n",
        "\n",
        "**After playing each arm once, do:**\n",
        "\n",
        "$A_t = \\arg\\max_{a\\in\\{1,\\dots,K\\}} F_{t-1}(a)$\n",
        "\n",
        "where the score or *index* of an action is\n",
        "\n",
        "$F_t(a) = \\widehat{\\mu}_a(t) + \\sqrt{\\frac{\\alpha \\log t}{N_a(t)}}$               \n",
        "\n",
        "and $N_a(t)$ is the number of times arm $a$ was selected up to time $t$ and $\\alpha>0$ is an exploration parameter. In the following code, only the action selection part is missing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5592d30",
      "metadata": {
        "id": "b5592d30"
      },
      "outputs": [],
      "source": [
        "class UCB(Agent):\n",
        "  \"\"\"\n",
        "  UCB (Upper Confidence Bound) MAB algorithm\n",
        "\n",
        "  :param K: number of arms\n",
        "  :param alpha: scaling of the optimistic bonus (appears under square root)\n",
        "  \"\"\"\n",
        "  def __init__(self, K, alpha):\n",
        "    self.alpha = alpha\n",
        "    self.K = K\n",
        "    self.reset()\n",
        "\n",
        "    self.cumulative_reward = np.zeros(self.K)\n",
        "    self.num_played = np.zeros(self.K)\n",
        "    self.avg_rewards = np.zeros(self.K)\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Reset history before each independent run\"\"\"\n",
        "    self.t = 0\n",
        "    self.avg_rewards = np.zeros(self.K)\n",
        "    self.num_played = np.zeros(self.K)\n",
        "    self.cumulative_reward = np.zeros(self.K)\n",
        "\n",
        "\n",
        "  def get_action(self):\n",
        "    \"\"\"\n",
        "    Select the arm to play based on current history\n",
        "\n",
        "    :return: the selected arm (an integer between 0 and K-1)\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    # TODO: Implement the action scores of UCB\n",
        "\n",
        "    scores = ... # K-dimensional ndarray\n",
        "\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "    chosen_arm_index = np.argmax(scores)\n",
        "    return chosen_arm_index\n",
        "\n",
        "  def receive_reward(self, chosen_arm, reward):\n",
        "    \"\"\"\n",
        "    Update history after playing an arm (counts and average rewards)\n",
        "\n",
        "    :param chosen_arm: the arm that was played\n",
        "    :param reward: the reward that was received\n",
        "    \"\"\"\n",
        "    self.cumulative_reward[chosen_arm] += reward\n",
        "    self.num_played[chosen_arm] += 1\n",
        "    self.avg_rewards[chosen_arm] = self.cumulative_reward[chosen_arm]/self.num_played[chosen_arm]\n",
        "\n",
        "    self.t += 1\n",
        "\n",
        "  def name(self):\n",
        "    return 'UCB('+str(self.alpha)+')'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PCEkCPM-GiF1",
      "metadata": {
        "id": "PCEkCPM-GiF1"
      },
      "source": [
        "üöÄ It's time to test your implementation of UCB!\n",
        "* Can you find a good value of $\\alpha$? Emilie's slides may help\n",
        "* How does it perform compared to $\\epsilon$-greedy?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d010cc34",
      "metadata": {
        "id": "d010cc34"
      },
      "outputs": [],
      "source": [
        "ucb = UCB(K, alpha=...) # Pick your alpha!\n",
        "\n",
        "epsilon_greedy = solutions.EpsilonGreedy(K,  eps=solutions.EPSILON)\n",
        "ucb_experiment = experiment_mab(env, [ucb, epsilon_greedy], N=N, T=T, mode=\"pseudo regret\")\n",
        "plot_result(ucb_experiment, q=10, mode=\"pseudo regret\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95d131f9",
      "metadata": {
        "id": "95d131f9"
      },
      "source": [
        "## Exercise 1.C &mdash; Thompson Sampling\n",
        "\n",
        "**tl;dr**\n",
        "> $F_{t-1,a} \\sim N\\left(\\widehat{\\mu}_a(t-1), \\frac{\\sigma^2}{N_a(t-1)}\\right)$\n",
        ">\n",
        "> $A_t = \\arg\\max_{a\\in\\{1,\\dots,K\\}} F_{t-1}(a)$\n",
        "\n",
        "üîß You next task is to implement the action-selection rule of Thompson Sampling (TS).\n",
        "\n",
        "### üìñ Gaussian Thompson Sampling\n",
        "\n",
        "TS is a *randomized* algorithm. We can still write the selection rule as for index algorithms:\n",
        "\n",
        "$A_t = \\arg\\max_{a\\in\\{1,\\dots,K\\}} F_{t-1}(a)$\n",
        "\n",
        "but now $F_{t}(a)$ is a randomized quantity:\n",
        "\n",
        "$F_{t}(a) \\sim Q_{t,a}$\n",
        "\n",
        "where $Q_{t,a}$ is a *posterior distribution* obtained by Bayes' rule:\n",
        "\n",
        "$Q_{t,a}(F) \\propto Q_{t-1,a}(F) \\cdot P(R_t | F)$\n",
        "\n",
        "starting from some given *prior* $Q_{0,a}$ and sequentially updating it (only for the chosen arm) with the *likelihood* of observed rewards (separately for each arm)\n",
        "\n",
        "In our case *we know* the rewards are gaussian with standard deviation $\\sigma=1$, so it is natural to use a Gaussian likelihood:\n",
        "\n",
        "$P(R_t|\\mu_a=F) = N(F,1)$\n",
        "\n",
        "and a zero-mean Gaussian prior:\n",
        "\n",
        "$Q_{0,a} = N(0,1)$ for all $a$.\n",
        "\n",
        "By Bayes' rule it's easy to show that the posterior is also Gaussian\n",
        "\n",
        "$Q_{t,a} = N\\left(\\widehat{\\mu}_a(t-1), \\frac{\\sigma^2}{N_a(t-1)}\\right)$\n",
        "\n",
        "where $\\sigma=1$ in our case. So you can implement the selection rule of Gaussian TS as follows:\n",
        "\n",
        "* For each arm $a\\in\\{1,\\dots,K\\}$ sample $F_{t-1,a} \\sim N\\left(\\widehat{\\mu}_a(t-1), \\frac{\\sigma^2}{N_a(t-1)}\\right)$\n",
        "* Play $A_t = \\arg\\max_{a\\in\\{1,\\dots,K\\}} F_{t-1}(a)$\n",
        "\n",
        "üéÅ [Hint](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html)\n",
        "\n",
        "üí™ **Bonus question:** what happens if you use a different prior, for instance a different value of `sigma` ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22e51e71",
      "metadata": {
        "id": "22e51e71"
      },
      "outputs": [],
      "source": [
        "class ThompsonSampling(Agent):\n",
        "  \"\"\"\n",
        "  Thompson Sampling MAB algorithm for Gaussian rewards\n",
        "\n",
        "  :param K: number of arms\n",
        "  :param sigma: standard deviation of Gaussian prior\n",
        "  \"\"\"\n",
        "  def __init__(self, K, sigma=1.):\n",
        "    self.sigma = sigma\n",
        "    self.K = K\n",
        "    self.reset()\n",
        "\n",
        "    self.cumulative_reward = np.zeros(self.K)\n",
        "    self.num_played = np.zeros(self.K)\n",
        "    self.avg_rewards = np.zeros(self.K)\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Reset history before each independent run\"\"\"\n",
        "    self.t = 0\n",
        "    self.avg_rewards = np.zeros(self.K)\n",
        "    self.cumulative_reward = np.zeros(self.K)\n",
        "    self.num_played = np.zeros(self.K)\n",
        "\n",
        "  def get_action(self):\n",
        "    \"\"\"\n",
        "    Select the arm to play based on current history\n",
        "\n",
        "    :return: the selected arm (an integer between 0 and K-1)\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    # TODO: implement the random action scores of Gaussian Thompson Sampling\n",
        "\n",
        "    scores = ...\n",
        "\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "    chosen_arm_index = np.argmax(scores)\n",
        "    return chosen_arm_index\n",
        "\n",
        "  def receive_reward(self, chosen_arm, reward):\n",
        "    \"\"\"\n",
        "    Update history after playing an arm (counts and average rewards)\n",
        "\n",
        "    :param chosen_arm: the arm that was played\n",
        "    :param reward: the reward that was received\n",
        "    \"\"\"\n",
        "    self.cumulative_reward[chosen_arm] += reward\n",
        "    self.num_played[chosen_arm] += 1\n",
        "    self.avg_rewards[chosen_arm] = self.cumulative_reward[chosen_arm]/self.num_played[chosen_arm]\n",
        "\n",
        "    self.t += 1\n",
        "\n",
        "  def name(self):\n",
        "    return 'TS('+str(self.sigma)+')'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bkT82Qujk5Kn",
      "metadata": {
        "id": "bkT82Qujk5Kn"
      },
      "source": [
        "üöÄ Now it's time to test your implementation of Thompson sampling on our 3-arm bandit problem. Does it perform better than $\\epsilon$-greedy?\n",
        "\n",
        "And of UCB?\n",
        "\n",
        "ü§ì If you don't see much difference among the different algorithms, you can try with a longer horizon $T$\n",
        "\n",
        "  Different results every time you run it? Try with a larger number of runs $N$ or seed numpy's random number generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e5ee401",
      "metadata": {
        "id": "0e5ee401"
      },
      "outputs": [],
      "source": [
        "ts = ThompsonSampling(K)\n",
        "\n",
        "epsilon_greedy = solutions.EpsilonGreedy(K,  eps=solutions.EPSILON)\n",
        "ucb = solutions.UCB(K, alpha=solutions.ALPHA)\n",
        "ts_experiment = experiment_mab(env, [epsilon_greedy, ucb, ts], N=N, T=T, mode=\"pseudo regret\")\n",
        "plot_result(ts_experiment, q=10, mode=\"pseudo regret\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd082727",
      "metadata": {
        "id": "fd082727"
      },
      "source": [
        "# Part 2 &mdash; Linear Contextual Bandits\n",
        "\n",
        "Coming back to our web application, we now want to develop a *personalised* system, taking into account that differnt users may like different versions of the website. We assume that for each pair of user features $u$ and website version $w$, there is a feature map $\\phi(u,w) \\in \\mathcal{R}^d$, such that the reward (engagement score) we get from user $U_t$ interacting with website version $W_t$ is\n",
        "\n",
        "$R_t=\\phi(U_t,W_t)^\\top \\theta + \\epsilon_t$\n",
        "\n",
        "where $\\epsilon_t \\sim  N(0, \\sigma^2)$ is just standard normal noise.\n",
        "\n",
        "This is an instance of the *linear bandit* problem introduced in Claire's lecture. The \"actions\" are now vectors in $R^d$, representing *contextual features* $X_t = \\phi(u_t,w)$, and the environment is assumed to generate rewards according to the unknown linear function $R_t = X_t^\\top \\theta + \\epsilon_t$ (parameter $\\theta\\in\\mathbb{R}^d$ is unknown to the agent)\n",
        "\n",
        "You can imagine that at each time step $t$ our website compares the current user's \"type\" (based on features like age, location...) with some properties of the $K=3$ versions of our webpage and produces $K$ feature vectors in $\\mathbb{R}^d$ ($d=7$), representing the estimated \"affinity\" of the current user with each of the $K$ versions. There are only $2$ user types in our example below.\n",
        "\n",
        "The underlying assumption is that these feature vectors are expressive enough so that the reward will be a linear function of these known \"actions\" with the unknown parameter $\\theta$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39nBLCnw3pHK",
      "metadata": {
        "id": "39nBLCnw3pHK"
      },
      "source": [
        "Here is some code to run linear-bandit experiments, with a similar interface to the one we have used for MAB. We will visualize **pseudo-regret**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90a2ddb9",
      "metadata": {
        "id": "90a2ddb9"
      },
      "outputs": [],
      "source": [
        "class LinearBandit(Environment):\n",
        "  \"\"\"\n",
        "  (Contextual) Linear Bandit instance\n",
        "\n",
        "  :param theta: unknown d-dimensional parameter of linear rewards\n",
        "  :param K: number of arms (feature vectors) per round\n",
        "  :param n_contexts: total number of possible contexts (different feature vectors per arm per round)\n",
        "  :param var: standard deviation of Gaussian noise\n",
        "  \"\"\"\n",
        "  def __init__(self, theta, K, n_contexts, var=1.):\n",
        "      self.d = np.size(theta)\n",
        "      self.theta = theta\n",
        "      self.K = K\n",
        "      self.var = var\n",
        "      self.n_contexts = n_contexts\n",
        "\n",
        "      self.features = np.random.multivariate_normal(np.zeros(d), np.eye(d), size=(self.n_contexts, K))\n",
        "      self.features = self.features\n",
        "      norms = np.linalg.norm(self.features, axis=2)\n",
        "      self.features = self.features / norms[:, :, np.newaxis]\n",
        "      current_context_id = np.random.randint(self.n_contexts)\n",
        "      self.current_action_set = self.features[current_context_id, :, :]\n",
        "\n",
        "  def get_action_set(self):\n",
        "      \"\"\"Returns the set of available arms (feature vectors) for the current round\"\"\"\n",
        "      current_context_id = np.random.randint(self.n_contexts)\n",
        "      self.current_action_set = self.features[current_context_id, :, :]\n",
        "      return self.current_action_set\n",
        "\n",
        "  def get_reward(self, action):\n",
        "      \"\"\"\n",
        "      Sample a random reward\n",
        "\n",
        "      :param action: the arm (d-dimensional feature vector) chosen by the learner)\n",
        "      \"\"\"\n",
        "      mean = np.dot(action, self.theta)\n",
        "      return np.random.normal(mean, scale=self.var)\n",
        "\n",
        "  def get_means(self):\n",
        "      \"\"\"Returns the mean rewards of all the available arms\"\"\"\n",
        "      return np.dot(self.current_action_set, self.theta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a32d7bfc",
      "metadata": {
        "id": "a32d7bfc"
      },
      "outputs": [],
      "source": [
        "def play_linb(environment, agent, N, T):\n",
        "    \"\"\"\n",
        "    Play N independent runs of length T for the specified agent.\n",
        "\n",
        "    :param environment: a linear bandit instance\n",
        "    :param agent: a contextual bandit algorithm\n",
        "    :param N: number of independent simulations\n",
        "    :param T: decision horizon\n",
        "    :return: the agent's name, and the collected data in an ndarray\n",
        "    \"\"\"\n",
        "\n",
        "    data = np.zeros((N, T))\n",
        "\n",
        "\n",
        "    for n in range(N):\n",
        "        agent.reset()\n",
        "        for t in range(T):\n",
        "            action_set = environment.get_action_set()\n",
        "            action = agent.get_action(action_set)\n",
        "            # Note that, differently from the previous part, now get_action needs to receive the action_set\n",
        "            reward = environment.get_reward(action)\n",
        "            agent.receive_reward(action,reward)\n",
        "\n",
        "            # Compute instant (pseudo) regret\n",
        "            means = environment.get_means()\n",
        "            best_reward = np.max(means)\n",
        "            data[n,t]= best_reward - np.dot(action, environment.theta) # pseudo regret\n",
        "\n",
        "    return agent.name(), data\n",
        "\n",
        "\n",
        "def experiment_linb(environment, agents, N, T):\n",
        "    \"\"\"\n",
        "    Play N trajectories for all agents over a horizon T. Store data in a dictionary.\n",
        "\n",
        "    :param environment: a linear bandit instance\n",
        "    :param agent: a list of contextual bandit algorithms to compare\n",
        "    :param N: number of independent simulations\n",
        "    :param T: decision horizon\n",
        "    :return: the pseudo regret for each agent in a dictionary indexed by the agent's name\n",
        "    \"\"\"\n",
        "\n",
        "    all_data = {}\n",
        "\n",
        "    for agent in agents:\n",
        "        agent_id, regrets = play_linb(environment, agent, N, T)\n",
        "\n",
        "        all_data[agent_id] = regrets\n",
        "\n",
        "    return all_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the specification of our linear bandit problem"
      ],
      "metadata": {
        "id": "lKDKPFgCuHIE"
      },
      "id": "lKDKPFgCuHIE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "078d3a7b",
      "metadata": {
        "id": "078d3a7b"
      },
      "outputs": [],
      "source": [
        "d = 7  # Feature dimension\n",
        "K = 3 # Number of arms per timestep (number of website versions)\n",
        "n_contexts = 2 # Number of contexts (types of users)\n",
        "theta = np.random.normal(0., 1., size=d)\n",
        "theta = theta / np.linalg.norm(theta)\n",
        "\n",
        "T = 1000  # Finite Horizon\n",
        "N = 50  # Monte Carlo simulations\n",
        "\n",
        "# The parameter theta is unknown, but we know it's been normalized (the l2 norm of theta is 1)\n",
        "# Feature vectors are also normalized\n",
        "lin_env = LinearBandit(theta, K, n_contexts)\n",
        "\n",
        "\n",
        "# Save subsampled points for Figures\n",
        "Nsub = 100\n",
        "tsav = range(2, T, Nsub)\n",
        "# Choice of percentile display\n",
        "q = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11b3cdc0",
      "metadata": {
        "id": "11b3cdc0"
      },
      "source": [
        "## Exercise 2.A &mdash; Linear $\\epsilon$-greedy\n",
        "\n",
        "**tl;dr**\n",
        "> $X_t = \\arg \\max_{x \\in \\mathcal{X}_t}\\left\\{ x^T \\hat{\\theta}_t\\right\\} $ with probability $1-\\epsilon$\n",
        ">\n",
        "> $X_t \\sim \\mathrm{Uniform}(\\mathbb{X}_t)$ otherwise\n",
        "\n",
        "üîß Your next task is to complete the implementation of the action selection rule of Linear $\\epsilon$-greedy. You can reuse some of your code from your implementation of $\\epsilon$-greedy for MABs.\n",
        "\n",
        "üëÅÔ∏è The main difference compared to the MAB case is that `get_action` has an input `arms` ($\\mathbb{X}_t$ in the pseudocode below) that is the set of $d$-dimensional feature vectors generated by the environment at time $t$.\n",
        "\n",
        "üìñ The selection rule is:\n",
        "\n",
        "* Play $X_t = \\arg \\max_{x \\in \\mathbb{X}_t} x^T \\hat{\\theta}_t $ with probability $1-\\epsilon$\n",
        "* Play $X_t$ sampled uniformly at random from $\\mathbb{X}_t$ instead\n",
        "\n",
        "where $\\hat{\\theta}_t$ is the *least squares estimate* of the unknown parameter $\\theta$ based on all the arms played so far and the corresponding collected rewards:\n",
        "\n",
        "$\\widehat{\\theta}_t = V_{t}^{-1}b_t$\n",
        "\n",
        "where $b_t$ is the *target vector*\n",
        "\n",
        "$b_t=\\sum_{s=1}^{t-1}X_{t}R_{t}$\n",
        "\n",
        "and $V_{t}$ is the $\\lambda$-regularized *covariance matrix*\n",
        "\n",
        "$V_{t}=\\lambda I + \\sum_{s=1}^{t-1}X_{t}X_{t}^T$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üéÅ [Hint](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html)"
      ],
      "metadata": {
        "id": "7waAH5xEzA2E"
      },
      "id": "7waAH5xEzA2E"
    },
    {
      "cell_type": "markdown",
      "id": "s1fbh7oKn5SO",
      "metadata": {
        "id": "s1fbh7oKn5SO"
      },
      "source": [
        "üí™ **Bonus exercise:** instead of inverting the covariance matrix at each step, you could update it incrementally using the [Sherman-Morrison formula](https://en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula). You can reuse this trick for LinUCB below (any practical implementation of LinUCB should use this implementation!)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ddbace5",
      "metadata": {
        "id": "4ddbace5"
      },
      "outputs": [],
      "source": [
        "class LinEpsilonGreedy(Agent):\n",
        "  \"\"\"\n",
        "  Linear epsilon-Greedy contextual-bandit algorithm\n",
        "\n",
        "  :param d: feature dimension\n",
        "  :param lambda_reg: regularization parameter for regularized least-squares\n",
        "  :eps: exploration probability\n",
        "  \"\"\"\n",
        "  def __init__(self, d,lambda_reg=1., eps=0.1,):\n",
        "    self.eps = eps # exploration probability\n",
        "    self.d = d\n",
        "    self.lambda_reg = lambda_reg\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Reset history before each independent run\"\"\"\n",
        "    self.t = 0\n",
        "\n",
        "    # The covariance matrix is initialized here\n",
        "    self.cov = self.lambda_reg * np.identity(self.d)\n",
        "\n",
        "    # The inverse of the covariance matrix is initialized here\n",
        "    self.invcov = np.identity(self.d)\n",
        "\n",
        "    # The target vector is initialized here\n",
        "    self.b_t = np.zeros(self.d)\n",
        "\n",
        "    # The parameter estimate is initialized here\n",
        "    self.hat_theta = np.zeros(self.d)\n",
        "\n",
        "\n",
        "  def get_action(self, arms):\n",
        "    \"\"\"\n",
        "    Select the arm (feature vector) to play based on current history\n",
        "\n",
        "    :param arms: the set of available arms (feature vectors)\n",
        "    :return: the selected arm (a d-dimensional feature vector)\n",
        "    \"\"\"\n",
        "    K, _ = arms.shape\n",
        "\n",
        "    u = np.random.random()\n",
        "    if u<self.eps:\n",
        "        return arms[np.random.choice(K)]\n",
        "    else:\n",
        "        # YOUR CODE HERE\n",
        "        # TODO: add the \"greedy\" action selection rule\n",
        "\n",
        "        return arms[0] # REPLACE THIS!\n",
        "\n",
        "        # END OF YOUR CODE\n",
        "\n",
        "  def receive_reward(self, chosen_arm, reward):\n",
        "    \"\"\"\n",
        "    Update history after playing an arm (least squares estimate)\n",
        "\n",
        "    :param chosen_arm: the arm (feature vector) that was played\n",
        "    :param reward: the reward that was received\n",
        "    \"\"\"\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    # TODO: Update the covariance matrix, its inverse, the target vector, and the parameter estimate\n",
        "\n",
        "    self.cov = ...\n",
        "\n",
        "    self.invcov = ...\n",
        "\n",
        "    self.b_t = ...\n",
        "\n",
        "    self.hat_theta = ...\n",
        "\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "    self.t += 1\n",
        "\n",
        "  def name(self):\n",
        "    return 'LinEGreedy('+str(self.eps)+')'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KAQgvkmpsCNE",
      "metadata": {
        "id": "KAQgvkmpsCNE"
      },
      "source": [
        "You can test Linear $\\epsilon$-greedy here.\n",
        "\n",
        "ü§ì Don't forget to compare it with the greedy version ($\\epsilon=0$)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0XqRaty7sQoA",
      "metadata": {
        "id": "0XqRaty7sQoA"
      },
      "outputs": [],
      "source": [
        "lin_eps_greedy = LinEpsilonGreedy(d, eps=0.1)\n",
        "\n",
        "lin_greedy = solutions.LinEpsilonGreedy(d, eps=0.)\n",
        "lin_eps_greedy_experiment = experiment_linb(lin_env, [lin_eps_greedy, lin_greedy], N=N, T=T)\n",
        "plot_result(lin_eps_greedy_experiment);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nOOI8ipvllF9",
      "metadata": {
        "id": "nOOI8ipvllF9"
      },
      "source": [
        "## Exercise 2.B &mdash; LinUCB\n",
        "\n",
        "**tl;dr**\n",
        "> $X_t = \\arg \\max_{x \\in \\mathbb{X}_t}\\left\\{ x^T \\widehat{\\theta_t} + \\alpha\\beta_t \\sqrt{x^T V_t^{-1} x}\\right\\}$\n",
        "\n",
        "üîß Time to implement LinUCB! You can reuse your implementation of `receive_reward` from `LinEpsilonGreedy`.\n",
        "\n",
        "üìñ The selection rule of LinUCB is:\n",
        "\n",
        "* $F_t(x) = x^T \\widehat{\\theta_t} + \\alpha\\beta_t \\sqrt{x^T V_t^{-1} x}$\n",
        "\n",
        "* Play $X_t = \\arg\\max_{x\\in\\mathbb{X}} F_t(x)$\n",
        "\n",
        "where we have already coded the \"theoretical\" exploration parameter $\\beta_t$ for you (compared to Claire's slides, we have $S=L=1$ here)\n",
        "\n",
        "$\\beta_t = \\sqrt{\\lambda} + \\sqrt{2\\log\\left(\\frac{1}{\\delta}\\right)+d\\log\\left(1+\\frac{t}{\\lambda d}\\right)}$\n",
        "\n",
        "ü§ì You can use the hyper-parameter $\\alpha$ to rescale the exploration bonus (smaller exploration bonuses may work better in practice...)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OnUAIx4liT0U",
      "metadata": {
        "id": "OnUAIx4liT0U"
      },
      "source": [
        "üí™ **Bonus questions**:\n",
        "* Can you use a smaller exploration parameter $\\beta_t$ in such a way that the regret guarantees of LinUCB are preserved? You can find the right idea in Claire's slides.\n",
        "* Play with different values of $d$, $K$, and n_contexts. What happens when the feature dimension $d$ is much smaller in comparison?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4745336a",
      "metadata": {
        "id": "4745336a"
      },
      "outputs": [],
      "source": [
        "class LinUCB(Agent):\n",
        "  \"\"\"\n",
        "  LinUCB (Linear Upper Confidence Bound) contextual-bandit algorithm for subgaussian rewards\n",
        "\n",
        "  :param d: feature dimension\n",
        "  :param delta: failure probability\n",
        "  :param lambda_reg: regularization parameter for regularized least-squares\n",
        "  :alpha: heuristic scaling of the exploration bonus\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, d, delta, lambda_reg, alpha=1.):\n",
        "    self.d = d\n",
        "    self.delta = delta\n",
        "    self.lambda_reg = lambda_reg\n",
        "    self.cov = self.lambda_reg * np.identity(d)\n",
        "\n",
        "\n",
        "    self.alpha = alpha\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Reset history before each independent run\"\"\"\n",
        "    self.t = 0\n",
        "    self.hat_theta = np.zeros(self.d)\n",
        "    self.cov = self.lambda_reg * np.identity(self.d)\n",
        "    self.invcov = np.identity(self.d)\n",
        "    self.b_t = np.zeros(self.d)\n",
        "\n",
        "\n",
        "  def get_action(self, arms):\n",
        "    \"\"\"\n",
        "    Select the arm (feature vector) to play based on current history\n",
        "\n",
        "    :param arms: the set of available arms (feature vectors)\n",
        "    :return: the selected arm (a d-dimensional feature vector)\n",
        "    \"\"\"\n",
        "    K, _ = arms.shape\n",
        "    self.UCBs = np.zeros(K)\n",
        "\n",
        "    self.beta = np.sqrt(self.lambda_reg) + np.sqrt(2*np.log(1./self.delta) + np.log(1+self.t/(self.d*self.lambda_reg)))\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    # TODO: Compute the action scores (self.UCBs)\n",
        "\n",
        "    ...\n",
        "\n",
        "    # END OF YOUR CODE\n",
        "\n",
        "    chosen_arm_index = np.argmax(self.UCBs)\n",
        "    chosen_arm = arms[chosen_arm_index]\n",
        "    return chosen_arm\n",
        "\n",
        "\n",
        "  def receive_reward(self, chosen_arm, reward):\n",
        "    \"\"\"\n",
        "    Update history after playing an arm (least squares estimate)\n",
        "\n",
        "    :param chosen_arm: the arm (feature vector) that was played\n",
        "    :param reward: the reward that was received\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    #TODO: same as Linear epsilon-Greedy\n",
        "\n",
        "    ...\n",
        "\n",
        "    # END OF YOUR CODE\n",
        "    self.t += 1\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "  def name(self):\n",
        "    return \"LinUCB(\"+str(self.alpha)+')'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4FkSfkxYlfIc",
      "metadata": {
        "id": "4FkSfkxYlfIc"
      },
      "source": [
        "üöÄ Time to test your implementation of LinUCB! Is it better than ($\\epsilon$-)greedy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "520f65bb",
      "metadata": {
        "id": "520f65bb"
      },
      "outputs": [],
      "source": [
        "linucb = LinUCB(d, delta=0.2, lambda_reg=1., alpha=1.) #You can try alpha < 1\n",
        "\n",
        "linear_epsilon_greedy = solutions.LinEpsilonGreedy(d, eps=0.)\n",
        "lin_ucb_experiment = experiment_linb(lin_env, [linear_epsilon_greedy, linucb], N=N, T=5*T)\n",
        "plot_result(lin_ucb_experiment);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aedc8c15",
      "metadata": {
        "id": "aedc8c15"
      },
      "source": [
        "## Exercise 2.C (üí™ BONUS) &mdash; LinTS\n",
        "\n",
        "Implement linear TS (see Claire's slides). You can find our (her) solution in `solutions.LinTS`. How does it perform compared to LinUCB?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc851033",
      "metadata": {
        "id": "bc851033"
      },
      "outputs": [],
      "source": [
        "class LinTS(Agent):\n",
        "  \"\"\"\n",
        "  LinTS (Linear Thompson Sampling) contextual-bandit algorithm for Gaussian rewards\n",
        "\n",
        "  :param d: feature dimension\n",
        "  :param delta: failure probability\n",
        "  :param lambda_reg: regularization parameter for regularized least-squares\n",
        "  :alpha: heuristic scaling of the exploration bonus\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, d, delta, lambda_prior):\n",
        "    self.d = d\n",
        "    self.delta = delta\n",
        "    self.lambda_prior = lambda_prior\n",
        "    self.cov = self.lambda_prior * np.identity(d)\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Reset history before each independent run\"\"\"\n",
        "    self.t = 0\n",
        "    self.hat_theta = np.zeros(self.d)\n",
        "    self.cov = self.lambda_prior * np.identity(self.d)\n",
        "    self.invcov = np.identity(self.d)\n",
        "    self.b_t = np.zeros(self.d)\n",
        "\n",
        "\n",
        "  def get_action(self, arms):\n",
        "    \"\"\"\n",
        "    Select the arm (feature vector) to play based on current history\n",
        "\n",
        "    :param arms: the set of available arms (feature vectors)\n",
        "    :return: the selected arm (a d-dimensional feature vector)\n",
        "    \"\"\"\n",
        "    K, _ = arms.shape\n",
        "    estimated_means = np.zeros(K)\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    # TODO: Compute the mean estimates\n",
        "    ...\n",
        "\n",
        "    # END OF CODE SOLUTION\n",
        "\n",
        "    chosen_arm_index = np.argmax(estimated_means)\n",
        "    chosen_arm = arms[chosen_arm_index]\n",
        "\n",
        "\n",
        "    return chosen_arm\n",
        "\n",
        "\n",
        "\n",
        "  def receive_reward(self, chosen_arm, reward):\n",
        "    \"\"\"\n",
        "    Update history after playing an arm (least squares estimate)\n",
        "\n",
        "    :param chosen_arm: the arm (feature vector) that was played\n",
        "    :param reward: the reward that was received\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    # TODO: same as Linear epsilon-Greedy\n",
        "    ...\n",
        "\n",
        "    # END OF CODE SOLUTION\n",
        "\n",
        "    self.t += 1\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "  def name(self):\n",
        "    return \"LinTS\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you can test your implementation of LinTS"
      ],
      "metadata": {
        "id": "PZTk_u9pg_qT"
      },
      "id": "PZTk_u9pg_qT"
    },
    {
      "cell_type": "code",
      "source": [
        "lints = LinTS(d, 0.2, 1.)\n",
        "\n",
        "linucb = solutions.LinUCB(d, delta=0.2, lambda_reg=1., alpha=1.)\n",
        "lin_ts_experiment = experiment_linb(lin_env, [linucb, lints], N=N, T=T)\n",
        "plot_result(lin_ts_experiment);"
      ],
      "metadata": {
        "id": "zTPWgytO_Bcv"
      },
      "id": "zTPWgytO_Bcv",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}